{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list containing all folder names of the dataset\n",
    "\n",
    "folders = [\"business\", \"entertainment\", \"food\", \"graphics\", \"historical\", \n",
    "           \"medical\", \"politics\", \"space\", \"sport\", \"technologie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the current working dirctory to the dataset directory\n",
    "\n",
    "os.chdir(\"C:/Users/fetih.kaya/Desktop/dataset\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\Desktop\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Verifying whether the current directory changed to the desier directory or not\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating  a list call paths containg all the detailed paths for accessing the folders of the dataset \n",
    "\n",
    "paths = []\n",
    "for i in folders:\n",
    "    paths.append(os.getcwd()+'/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/business', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/entertainment', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/food', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/graphics', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/historical', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/medical', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/politics', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/space', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/sport', 'C:\\\\Users\\\\fetih.kaya\\\\Desktop\\\\dataset/technologie']\n"
     ]
    }
   ],
   "source": [
    "# printing the path just for verification\n",
    "\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two list texts and labels to store the detailed content and\n",
    "# and the category of the news\n",
    "\n",
    "texts = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the path of a folders one by one from the list called paths[]\n",
    "for path in paths:\n",
    "    \n",
    "#getting filenames one by one from the list of all files under a directory called path\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "#opening the file in read only mode with encoding latin1\n",
    "        with open(path+\"/\"+filename,\"r\", encoding = \"latin\") as file:\n",
    "        \n",
    "#after reading the file contents are stored in a variable called data\n",
    "            data = file.read()\n",
    "    \n",
    "#after removing all newline characters and carriage return from the  data,it is being added at the end of the list texts\n",
    "            data = data.replace(\"\\n\",\" \").replace('\\r',' ').replace('\\t',' ').replace('  ',' ').replace('   ',' ').replace('    ',' ')\n",
    "            texts.append(data)\n",
    "            file.close() # file is being closed\n",
    "            \n",
    "#setting the category of the news as the folder name in which it reside  \n",
    "        labels.append(os.path.basename(path)) # from the whole path basename only returns the folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pandas dataframe from two lists texts and labels as two columns\n",
    "\n",
    "df = pd.DataFrame({'category':labels, 'text':texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>Lufthansa flies back to profit German airline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Winn-Dixie files for bankruptcy US supermarket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>US economy still growing says Fed Most areas o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>business</td>\n",
       "      <td>Saab to build Cadillacs in Sweden General Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>business</td>\n",
       "      <td>Bank voted 8-1 for no rate change The decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Mobile games come of age The BBC News website ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>technologie</td>\n",
       "      <td>California sets fines for spyware The makers o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Web helps collect aid donations The web is hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Mobiles rack up 20 years of use Mobile phones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Blogs take on the mainstream Web logs or blogs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0       business  Lufthansa flies back to profit German airline ...\n",
       "1       business  Winn-Dixie files for bankruptcy US supermarket...\n",
       "2       business  US economy still growing says Fed Most areas o...\n",
       "3       business  Saab to build Cadillacs in Sweden General Moto...\n",
       "4       business  Bank voted 8-1 for no rate change The decision...\n",
       "..           ...                                                ...\n",
       "995  technologie  Mobile games come of age The BBC News website ...\n",
       "996  technologie  California sets fines for spyware The makers o...\n",
       "997  technologie  Web helps collect aid donations The web is hel...\n",
       "998  technologie  Mobiles rack up 20 years of use Mobile phones ...\n",
       "999  technologie  Blogs take on the mainstream Web logs or blogs...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the data frame for verification purpose\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Industrial revival hope for Japan Japanese industry is growing faster than expected, boosting hopes that the country\\'s retreat back into recession is over. Industrial output rose 2.1% - adjusted for the time of year - in January from a month earlier. At the same time, retail sales picked up faster than at any time since 1997. The news sent Tokyo shares to an eight-month high, as investors hoped for a recovery from the three quarters of contraction seen from April 2004 on. The Nikkei 225 index ended the day up 0.7% at 11,740.60 points, with the yen strengthening 0.7% against the dollar to 104.53 yen. Weaker exports, normally the engine for Japan\\'s economy in the face of weak domestic demand, had helped trigger a 0.1% contraction in the final three months of last year after two previous quarters of shrinking GDP. Only an exceptionally strong performance in the early months of 2004 kept the year as a whole from showing a decline. The output figures brought a cautiously optimistic response from economic officials. \"Overall I see a low risk of the economy falling into serious recession,\" said Bank of Japan chief Toshihiko Fukui, despite warning that other indicators - such as the growth numbers - had been worrying. Within the overall industrial output figure, there were signs of a pullback from the export slowdown. Among the best-performing sectors were key overseas sales areas such as cars, chemicals and electronic goods. With US growth doing better than expected the picture for exports in early 2005 could also be one of sustained demand. Electronics were also one of the keys to the improved domestic market, with products such as flat-screen TVs in high demand during January. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take an example from the dataset\n",
    "\n",
    "df.iloc[5,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOCUMENT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from warnings import simplefilter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>food</td>\n",
       "      <td>3 carrots -- large  1 turnip -- large  2  sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Roxy Music on Isle of Wight bill Roxy Music wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>technologie</td>\n",
       "      <td>'Evil twin' fear for wireless net People using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>sport</td>\n",
       "      <td>Gatlin and Hayes win Owen awards American Olym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>food</td>\n",
       "      <td>4  cloves garlic finely chopped  1 28 ounce c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>sport</td>\n",
       "      <td>Bekele sets sights on world mark Olympic 10,00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Apple laptop is 'greatest gadget' The Apple Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>food</td>\n",
       "      <td>Firat GÃƒÂ¼der admits it, albeit a bit chagrined...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>technologie</td>\n",
       "      <td>Domain system scam fear A system to make it ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>politics</td>\n",
       "      <td>'Debate needed' on donations cap A cap on dona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category                                               text\n",
       "238           food   3 carrots -- large  1 turnip -- large  2  sta...\n",
       "165  entertainment  Roxy Music on Isle of Wight bill Roxy Music wi...\n",
       "971    technologie  'Evil twin' fear for wireless net People using...\n",
       "836          sport  Gatlin and Hayes win Owen awards American Olym...\n",
       "231           food   4  cloves garlic finely chopped  1 28 ounce c...\n",
       "826          sport  Bekele sets sights on world mark Olympic 10,00...\n",
       "989    technologie  Apple laptop is 'greatest gadget' The Apple Po...\n",
       "259           food  Firat GÃƒÂ¼der admits it, albeit a bit chagrined...\n",
       "944    technologie  Domain system scam fear A system to make it ea...\n",
       "683       politics  'Debate needed' on donations cap A cap on dona..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         100\n",
       "graphics         100\n",
       "medical          100\n",
       "food             100\n",
       "space            100\n",
       "entertainment    100\n",
       "politics         100\n",
       "sport            100\n",
       "technologie      100\n",
       "historical       100\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Tuesday, June 22, 1993 Carderock Division, Naval Surface Warfare Center  (formerly the David Taylor Research Center) Bethesda, Maryland SPONSOR: NESS (Navy Engineering Software System) is sponsoring a one-day Navy Scientific Visualization and Virtual Reality Seminar.  The purpose of the seminar is to present and exchange information for Navy-related scientific visualization and virtual reality programs, research, developments, and applications. PRESENTATIONS: Presentations are solicited on all aspects of Navy-related scientific visualization and virtual reality. All current work, works-in-progress, and proposed work by Navy organizations will be considered. Four types of presentations are available. 1. Regular presentation: 20-30 minutes in length 2. Short presentation: 10 minutes in length 3. Video presentation: a stand-alone videotape (author need not  attend the seminar) 4. Scientific visualization or virtual reality demonstration (BYOH) Accepted presentations will not be published in any proceedings, however, viewgraphs and other materials will be reproduced for seminar attendees. ABSTRACTS: Authors should submit a one page abstract and/or videotape to: Robert Lipman Naval Surface Warfare Center, Carderock Division Code 2042 Bethesda, Maryland 20084-5000 E-MAIL lipman@oasys.dt.navy.mil Authors should include the type of presentation, their affiliations, addresses, telephone and FAX numbers, and addresses. Multi-author papers should designate one point of contact. DEADLINES: The abstact submission deadline is April 30, 1993.  Notification of acceptance will be sent by May 14, 1993.  Materials for reproduction must be received by June 1, 1993. For further information, contact Robert Lipman at the above address. Robert Lipman  | Internet: lipman@oasys.dt.navy.mil David Taylor Model Basin - CDNSWC |  or: lip@ocean.dt.navy.mil Computational Signatures and | Voicenet: (301) 227-3618  Structures Group, Code 2042  | Factsnet: (301) 227-5753 Bethesda, Maryland 20084-5000  | Phishnet: stockings@long.legs The sixth sick shiek's sixth sheep's sick. \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[300,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#producing train and test sets\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['text'],df[\"category\"],test_size = 0.2)\n",
    "train_index = x_train.index\n",
    "test_index = x_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.62      0.83      0.71        18\n",
      "entertainment       0.69      0.72      0.71        25\n",
      "         food       0.75      0.86      0.80        14\n",
      "     graphics       0.60      0.71      0.65        17\n",
      "   historical       0.85      0.92      0.88        24\n",
      "      medical       0.68      0.72      0.70        18\n",
      "     politics       0.88      0.68      0.77        22\n",
      "        space       1.00      0.75      0.86        20\n",
      "        sport       0.90      0.90      0.90        20\n",
      "  technologie       0.82      0.64      0.72        22\n",
      "\n",
      "     accuracy                           0.77       200\n",
      "    macro avg       0.78      0.77      0.77       200\n",
      " weighted avg       0.79      0.77      0.77       200\n",
      "\n",
      "[[15  2  0  0  1  0  0  0  0  0]\n",
      " [ 4 18  0  0  0  1  0  0  1  1]\n",
      " [ 0  0 12  2  0  0  0  0  0  0]\n",
      " [ 0  0  2 12  0  2  0  0  0  1]\n",
      " [ 0  0  1  0 22  0  1  0  0  0]\n",
      " [ 0  1  1  2  1 13  0  0  0  0]\n",
      " [ 2  1  0  1  2  0 15  0  1  0]\n",
      " [ 0  0  0  2  0  3  0 15  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 18  1]\n",
      " [ 3  3  0  1  0  0  1  0  0 14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Vector space modelling\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "count_vectorizer_model = vectorizer.fit(df[\"text\"])\n",
    "vectors = count_vectorizer_model.transform(df[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_1 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.50      0.72      0.59        18\n",
      "entertainment       0.81      0.84      0.82        25\n",
      "         food       0.89      0.57      0.70        14\n",
      "     graphics       0.34      0.71      0.46        17\n",
      "   historical       0.80      0.67      0.73        24\n",
      "      medical       0.73      0.61      0.67        18\n",
      "     politics       0.75      0.68      0.71        22\n",
      "        space       0.78      0.70      0.74        20\n",
      "        sport       0.95      0.95      0.95        20\n",
      "  technologie       0.64      0.32      0.42        22\n",
      "\n",
      "     accuracy                           0.68       200\n",
      "    macro avg       0.72      0.68      0.68       200\n",
      " weighted avg       0.72      0.68      0.69       200\n",
      "\n",
      "[[13  1  0  1  0  0  1  0  0  2]\n",
      " [ 2 21  0  1  0  0  1  0  0  0]\n",
      " [ 1  0  8  4  1  0  0  0  0  0]\n",
      " [ 1  0  0 12  0  0  0  4  0  0]\n",
      " [ 1  1  1  3 16  0  1  0  0  1]\n",
      " [ 0  0  0  7  0 11  0  0  0  0]\n",
      " [ 1  2  0  0  2  1 15  0  0  1]\n",
      " [ 0  0  0  3  0  3  0 14  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 19  0]\n",
      " [ 7  0  0  4  1  0  2  0  1  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Vector space modelling\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\")\n",
    "tfidf_model = vectorizer.fit(df[\"text\"])\n",
    "vectors = tfidf_model.transform(df[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_2 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3 - Count Vectorizer + StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.64      1.00      0.78        18\n",
      "entertainment       0.86      0.96      0.91        25\n",
      "         food       1.00      0.79      0.88        14\n",
      "     graphics       0.57      0.71      0.63        17\n",
      "   historical       1.00      0.83      0.91        24\n",
      "      medical       0.62      0.72      0.67        18\n",
      "     politics       0.90      0.82      0.86        22\n",
      "        space       0.76      0.80      0.78        20\n",
      "        sport       0.91      1.00      0.95        20\n",
      "  technologie       0.88      0.32      0.47        22\n",
      "\n",
      "     accuracy                           0.80       200\n",
      "    macro avg       0.81      0.79      0.78       200\n",
      " weighted avg       0.82      0.80      0.79       200\n",
      "\n",
      "[[18  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 24  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 11  2  0  0  0  1  0  0]\n",
      " [ 1  0  0 12  0  4  0  0  0  0]\n",
      " [ 0  1  0  0 20  0  0  2  0  1]\n",
      " [ 0  0  0  2  0 13  1  2  0  0]\n",
      " [ 3  0  0  1  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  4  0 16  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 20  0]\n",
      " [ 6  3  0  4  0  0  1  0  1  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Vector space modelling\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words='english')\n",
    "count_vectorizer_model = vectorizer.fit(df[\"text\"])\n",
    "vectors = count_vectorizer_model.transform(df[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_3 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4 - Tf - idf Vectorizer + StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.68      0.94      0.79        18\n",
      "entertainment       0.95      0.80      0.87        25\n",
      "         food       1.00      0.79      0.88        14\n",
      "     graphics       0.48      0.82      0.61        17\n",
      "   historical       0.91      0.88      0.89        24\n",
      "      medical       0.71      0.83      0.77        18\n",
      "     politics       0.95      0.86      0.90        22\n",
      "        space       0.89      0.80      0.84        20\n",
      "        sport       0.91      1.00      0.95        20\n",
      "  technologie       1.00      0.45      0.62        22\n",
      "\n",
      "     accuracy                           0.81       200\n",
      "    macro avg       0.85      0.82      0.81       200\n",
      " weighted avg       0.86      0.81      0.82       200\n",
      "\n",
      "[[17  1  0  0  0  0  0  0  0  0]\n",
      " [ 2 20  0  1  1  0  0  0  1  0]\n",
      " [ 0  0 11  1  0  1  1  0  0  0]\n",
      " [ 0  0  0 14  0  3  0  0  0  0]\n",
      " [ 0  0  0  2 21  1  0  0  0  0]\n",
      " [ 0  0  0  2  0 15  0  1  0  0]\n",
      " [ 0  0  0  1  1  0 19  1  0  0]\n",
      " [ 1  0  0  2  0  1  0 16  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 20  0]\n",
      " [ 5  0  0  6  0  0  0  0  1 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Vector space modelling\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", stop_words='english')\n",
    "tfidf_model = vectorizer.fit(df[\"text\"])\n",
    "vectors = tfidf_model.transform(df[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_4= accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 5 - Count Vectorizer + StopWords + Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.68      0.94      0.79        18\n",
      "entertainment       0.95      0.80      0.87        25\n",
      "         food       1.00      1.00      1.00        14\n",
      "     graphics       0.64      0.82      0.72        17\n",
      "   historical       0.95      0.88      0.91        24\n",
      "      medical       0.83      0.83      0.83        18\n",
      "     politics       0.95      0.91      0.93        22\n",
      "        space       0.76      0.80      0.78        20\n",
      "        sport       0.95      1.00      0.98        20\n",
      "  technologie       0.80      0.55      0.65        22\n",
      "\n",
      "     accuracy                           0.84       200\n",
      "    macro avg       0.85      0.85      0.85       200\n",
      " weighted avg       0.86      0.84      0.84       200\n",
      "\n",
      "[[17  0  0  0  0  0  0  0  0  1]\n",
      " [ 2 20  0  0  0  0  1  1  1  0]\n",
      " [ 0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  2  0  1  0  0]\n",
      " [ 0  0  0  1 21  0  0  1  0  1]\n",
      " [ 0  0  0  1  0 15  0  2  0  0]\n",
      " [ 0  0  0  1  0  0 20  0  0  1]\n",
      " [ 0  0  0  2  1  1  0 16  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 20  0]\n",
      " [ 6  1  0  3  0  0  0  0  0 12]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "    \n",
    "#Vector space modelling\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words='english')\n",
    "count_vectorizer_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = count_vectorizer_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_5 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 6 - Tf - idf Vectorizer + StopWords + Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.50      0.72      0.59        18\n",
      "entertainment       0.83      0.80      0.82        25\n",
      "         food       0.67      0.86      0.75        14\n",
      "     graphics       0.71      0.88      0.79        17\n",
      "   historical       1.00      0.79      0.88        24\n",
      "      medical       1.00      0.78      0.88        18\n",
      "     politics       0.90      0.86      0.88        22\n",
      "        space       0.80      0.80      0.80        20\n",
      "        sport       0.90      0.95      0.93        20\n",
      "  technologie       0.88      0.64      0.74        22\n",
      "\n",
      "     accuracy                           0.81       200\n",
      "    macro avg       0.82      0.81      0.81       200\n",
      " weighted avg       0.83      0.81      0.81       200\n",
      "\n",
      "[[13  2  0  0  0  0  2  0  0  1]\n",
      " [ 2 20  0  1  0  0  0  0  2  0]\n",
      " [ 1  0 12  1  0  0  0  0  0  0]\n",
      " [ 0  0  2 15  0  0  0  0  0  0]\n",
      " [ 1  2  0  1 19  0  0  1  0  0]\n",
      " [ 0  0  1  0  0 14  0  3  0  0]\n",
      " [ 2  0  0  1  0  0 19  0  0  0]\n",
      " [ 0  0  2  1  0  0  0 16  0  1]\n",
      " [ 0  0  1  0  0  0  0  0 19  0]\n",
      " [ 7  0  0  1  0  0  0  0  0 14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fetih.kaya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "\n",
    "#Vector space modelling\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", stop_words='english')\n",
    "tfidf_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = tfidf_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_6 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determination for parameters for Count Vectorizer, Tf-idf Vectorizer and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.940 (std: 0.011)\n",
      "Parameters: {'n_estimators': 400, 'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 50, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.939 (std: 0.015)\n",
      "Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.939 (std: 0.010)\n",
      "Parameters: {'n_estimators': 400, 'min_samples_split': 50, 'max_features': 'auto', 'max_depth': 50, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.932 (std: 0.017)\n",
      "Parameters: {'n_estimators': 200, 'min_samples_split': 20, 'max_features': 'sqrt', 'max_depth': 20, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.919 (std: 0.008)\n",
      "Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'max_features': 'log2', 'max_depth': 20, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 6\n",
      "Mean validation score: 0.902 (std: 0.021)\n",
      "Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': None, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 7\n",
      "Mean validation score: 0.901 (std: 0.016)\n",
      "Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_features': 'auto', 'max_depth': 5, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 8\n",
      "Mean validation score: 0.889 (std: 0.012)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 20, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 9\n",
      "Mean validation score: 0.851 (std: 0.005)\n",
      "Parameters: {'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 50, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 10\n",
      "Mean validation score: 0.840 (std: 0.011)\n",
      "Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 2, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 10\n",
      "Mean validation score: 0.840 (std: 0.004)\n",
      "Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'max_features': None, 'max_depth': 100, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 12\n",
      "Mean validation score: 0.838 (std: 0.019)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 2, 'max_features': 'auto', 'max_depth': 5, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 13\n",
      "Mean validation score: 0.835 (std: 0.012)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 2, 'max_features': 'auto', 'max_depth': 5, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 14\n",
      "Mean validation score: 0.816 (std: 0.022)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 5, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 15\n",
      "Mean validation score: 0.814 (std: 0.024)\n",
      "Parameters: {'n_estimators': 20, 'min_samples_split': 2, 'max_features': None, 'max_depth': 20, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 16\n",
      "Mean validation score: 0.787 (std: 0.033)\n",
      "Parameters: {'n_estimators': 10, 'min_samples_split': 50, 'max_features': None, 'max_depth': 50, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 17\n",
      "Mean validation score: 0.781 (std: 0.018)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 10, 'max_features': None, 'max_depth': 50, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 18\n",
      "Mean validation score: 0.780 (std: 0.031)\n",
      "Parameters: {'n_estimators': 400, 'min_samples_split': 2, 'max_features': None, 'max_depth': 10, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 19\n",
      "Mean validation score: 0.581 (std: 0.050)\n",
      "Parameters: {'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 2, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 20\n",
      "Mean validation score: 0.487 (std: 0.037)\n",
      "Parameters: {'n_estimators': 50, 'min_samples_split': 5, 'max_features': None, 'max_depth': 2, 'criterion': 'entropy'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random forest parameter selection\n",
    "\n",
    "def report(results, n_top=30): #print all results\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "#Random Forest random search\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"n_estimators\": [5, 10, 20, 50, 100, 200, 400],\n",
    "              \"min_samples_split\": [2, 5, 10, 20, 50],\n",
    "              \"max_depth\": [None, 2, 5, 10, 20, 50, 100],\n",
    "              \"max_features\": [None, \"auto\", \"sqrt\", \"log2\"]\n",
    "             }\n",
    "\n",
    "n_iter_search = 20\n",
    "\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=n_iter_search, cv=5)\n",
    "\n",
    "simplefilter('ignore')\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decode_error</th>\n",
       "      <th>strip_accents</th>\n",
       "      <th>max_df</th>\n",
       "      <th>max_features</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>replace</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>replace</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>strict</td>\n",
       "      <td>ascii</td>\n",
       "      <td>0.8</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>ignore</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>ignore</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>replace</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    decode_error strip_accents  max_df  max_features  accuracy\n",
       "709      replace       unicode     0.1           NaN     0.960\n",
       "108       strict       unicode     0.1       20000.0     0.955\n",
       "809      replace          None     0.1           NaN     0.955\n",
       "78        strict         ascii     0.8       20000.0     0.950\n",
       "409       ignore       unicode     0.1           NaN     0.950\n",
       "109       strict       unicode     0.1           NaN     0.950\n",
       "137       strict       unicode     0.4       10000.0     0.950\n",
       "509       ignore          None     0.1           NaN     0.945\n",
       "708      replace       unicode     0.1       20000.0     0.945\n",
       "119       strict       unicode     0.2           NaN     0.945"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count Vectorizer parameter selection\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "y = 0\n",
    "\n",
    "dataframe_1 = pd.DataFrame(np.zeros((900, 5)), columns=['decode_error', 'strip_accents',\n",
    "                                                      'max_df', 'max_features', 'accuracy'])\n",
    "\n",
    "decode_error = (\"strict\", \"ignore\", \"replace\")\n",
    "strip_accents = (\"ascii\", \"unicode\",None)\n",
    "max_df = ( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n",
    "max_features = ( 10, 20, 50, 100, 500, 1000, 5000, 10000, 20000, None)\n",
    "\n",
    "for i in decode_error:\n",
    "    for j in strip_accents:\n",
    "        for l in max_df:\n",
    "            for m in max_features:\n",
    "                #Count vectorizer modelling\n",
    "                vectorizer = CountVectorizer(analyzer = \"word\", decode_error=\"i\", strip_accents=j, \n",
    "                                                 max_df=l, max_features=m)\n",
    "                count_vectorizer_model = vectorizer.fit(df[\"text\"])\n",
    "                vectors = count_vectorizer_model.transform(df[\"text\"])\n",
    "\n",
    "                #Modelling\n",
    "                x_train = vectors[train_index,:]\n",
    "                y_train = df['category'][train_index]  \n",
    "                x_test  = vectors[test_index,:] \n",
    "                y_test  = df['category'][test_index]   \n",
    "\n",
    "                clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, \n",
    "                                             max_depth=50, max_features='sqrt')\n",
    "                clf.fit(x_train, y_train)\n",
    "                predictions = clf.predict(x_test)\n",
    "                    \n",
    "                dataframe_1.iloc[y,0] = i\n",
    "                dataframe_1.iloc[y,1] = j\n",
    "                dataframe_1.iloc[y,2] = l\n",
    "                dataframe_1.iloc[y,3] = m\n",
    "                dataframe_1.iloc[y,4] = accuracy_score(y_test, predictions)\n",
    "                    \n",
    "                y = y+1\n",
    "                    \n",
    "dataframe_1  = dataframe_1.sort_values([\"accuracy\"], ascending = False)  \n",
    "\n",
    "dataframe_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decode_error</th>\n",
       "      <th>strip_accents</th>\n",
       "      <th>max_df</th>\n",
       "      <th>max_feature</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>strict</td>\n",
       "      <td>ascii</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>replace</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>replace</td>\n",
       "      <td>ascii</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>strict</td>\n",
       "      <td>ascii</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>strict</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>strict</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>strict</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>ignore</td>\n",
       "      <td>unicode</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>ignore</td>\n",
       "      <td>ascii</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    decode_error strip_accents  max_df  max_feature  accuracy\n",
       "8         strict         ascii     0.1      20000.0     0.970\n",
       "109       strict       unicode     0.1          NaN     0.965\n",
       "718      replace       unicode     0.2      20000.0     0.965\n",
       "608      replace         ascii     0.1      20000.0     0.965\n",
       "38        strict         ascii     0.4      20000.0     0.965\n",
       "209       strict          None     0.1          NaN     0.965\n",
       "107       strict       unicode     0.1      10000.0     0.960\n",
       "219       strict          None     0.2          NaN     0.960\n",
       "438       ignore       unicode     0.4      20000.0     0.960\n",
       "319       ignore         ascii     0.2          NaN     0.960"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tf-idf Vectorizer parameter selection\n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "y = 0\n",
    "\n",
    "dataframe_2 = pd.DataFrame(np.zeros((900, 5)), columns=['decode_error', 'strip_accents',\n",
    "                                                      'max_df', 'max_feature', 'accuracy'])\n",
    "\n",
    "decode_error = (\"strict\", \"ignore\", \"replace\")\n",
    "strip_accents = (\"ascii\", \"unicode\",None)\n",
    "max_df = ( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n",
    "max_features = ( 10, 20, 50, 100, 500, 1000, 5000, 10000, 20000, None)\n",
    "\n",
    "for i in decode_error:\n",
    "    for j in strip_accents:\n",
    "        for l in max_df:\n",
    "            for m in max_features:\n",
    "                #Tf - idf vectorizer modelling\n",
    "                vectorizer = TfidfVectorizer(analyzer = \"word\", decode_error=\"i\", strip_accents=j, \n",
    "                                                 max_df=l, max_features=m)\n",
    "                tfidf_model = vectorizer.fit(df[\"text\"])\n",
    "                vectors = tfidf_model.transform(df[\"text\"])\n",
    "\n",
    "                #Modelling\n",
    "                x_train = vectors[train_index,:]\n",
    "                y_train = df['category'][train_index]  \n",
    "                x_test  = vectors[test_index,:] \n",
    "                y_test  = df['category'][test_index]   \n",
    "\n",
    "                clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, \n",
    "                                             max_depth=50, max_features='sqrt')\n",
    "                clf.fit(x_train, y_train)\n",
    "                predictions = clf.predict(x_test)\n",
    "                    \n",
    "                dataframe_2.iloc[y,0] = i\n",
    "                dataframe_2.iloc[y,1] = j\n",
    "                dataframe_2.iloc[y,2] = l\n",
    "                dataframe_2.iloc[y,3] = m\n",
    "                dataframe_2.iloc[y,4] = accuracy_score(y_test, predictions)\n",
    "                    \n",
    "                y = y+1\n",
    "                    \n",
    "dataframe_2  = dataframe_2.sort_values([\"accuracy\"], ascending = False)   \n",
    "\n",
    "dataframe_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 7 - Count Vectorizer + StopWords + Stemming + Random Forest Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      1.00      0.95        18\n",
      "entertainment       0.96      0.96      0.96        25\n",
      "         food       1.00      1.00      1.00        14\n",
      "     graphics       0.74      0.82      0.78        17\n",
      "   historical       1.00      0.96      0.98        24\n",
      "      medical       0.81      0.94      0.87        18\n",
      "     politics       0.92      1.00      0.96        22\n",
      "        space       1.00      0.85      0.92        20\n",
      "        sport       0.95      0.95      0.95        20\n",
      "  technologie       1.00      0.77      0.87        22\n",
      "\n",
      "     accuracy                           0.93       200\n",
      "    macro avg       0.93      0.93      0.92       200\n",
      " weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "[[18  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 24  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  3  0  0  0  0]\n",
      " [ 0  0  0  0 23  1  0  0  0  0]\n",
      " [ 0  0  0  1  0 17  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 22  0  0  0]\n",
      " [ 0  0  0  3  0  0  0 17  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 19  0]\n",
      " [ 2  0  0  1  0  0  2  0  0 17]]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "    \n",
    "#Vector space modelling\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words='english')\n",
    "count_vectorizer_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = count_vectorizer_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, max_depth=50, max_features='sqrt')\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_7 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 8 - Tf - idf Vectorizer + StopWords + Stemming + Random Forest Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      1.00      0.95        18\n",
      "entertainment       0.93      1.00      0.96        25\n",
      "         food       1.00      1.00      1.00        14\n",
      "     graphics       0.88      0.88      0.88        17\n",
      "   historical       1.00      0.96      0.98        24\n",
      "      medical       0.90      1.00      0.95        18\n",
      "     politics       0.96      1.00      0.98        22\n",
      "        space       0.95      0.90      0.92        20\n",
      "        sport       1.00      0.95      0.97        20\n",
      "  technologie       1.00      0.82      0.90        22\n",
      "\n",
      "     accuracy                           0.95       200\n",
      "    macro avg       0.95      0.95      0.95       200\n",
      " weighted avg       0.95      0.95      0.95       200\n",
      "\n",
      "[[18  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 15  0  1  0  1  0  0]\n",
      " [ 0  1  0  0 23  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 22  0  0  0]\n",
      " [ 0  0  0  1  0  1  0 18  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 19  0]\n",
      " [ 2  0  0  1  0  0  1  0  0 18]]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "\n",
    "#Vector space modelling\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", stop_words='english')\n",
    "tfidf_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = tfidf_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, max_depth=50, max_features='sqrt')\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_8 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 9 - Count Vectorizer + StopWords + Stemming + Random Forest Hyper-parameter Tuning + Count Vectorizer Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.86      1.00      0.92        18\n",
      "entertainment       1.00      1.00      1.00        25\n",
      "         food       1.00      1.00      1.00        14\n",
      "     graphics       0.74      1.00      0.85        17\n",
      "   historical       1.00      0.96      0.98        24\n",
      "      medical       1.00      1.00      1.00        18\n",
      "     politics       1.00      0.95      0.98        22\n",
      "        space       1.00      0.90      0.95        20\n",
      "        sport       1.00      0.95      0.97        20\n",
      "  technologie       1.00      0.82      0.90        22\n",
      "\n",
      "     accuracy                           0.95       200\n",
      "    macro avg       0.96      0.96      0.96       200\n",
      " weighted avg       0.96      0.95      0.96       200\n",
      "\n",
      "[[18  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 17  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 23  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  1  0  0 21  0  0  0]\n",
      " [ 0  0  0  2  0  0  0 18  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 19  0]\n",
      " [ 3  0  0  1  0  0  0  0  0 18]]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "    \n",
    "#Vector space modelling\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words='english', decode_error='replace', strip_accents='unicode', \n",
    "                             max_df=0.1, max_features=None)\n",
    "count_vectorizer_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = count_vectorizer_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, max_depth=50, max_features='sqrt')\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_9 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 10 - Tf - idf Vectorizer + StopWords + Stemming + Random Forest Hyper-parameter Tuning + Tf-idf Vectorizer Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      1.00      1.00        18\n",
      "entertainment       0.96      1.00      0.98        25\n",
      "         food       1.00      1.00      1.00        14\n",
      "     graphics       0.84      0.94      0.89        17\n",
      "   historical       1.00      0.96      0.98        24\n",
      "      medical       1.00      1.00      1.00        18\n",
      "     politics       1.00      1.00      1.00        22\n",
      "        space       1.00      0.90      0.95        20\n",
      "        sport       0.95      0.95      0.95        20\n",
      "  technologie       0.95      0.95      0.95        22\n",
      "\n",
      "     accuracy                           0.97       200\n",
      "    macro avg       0.97      0.97      0.97       200\n",
      " weighted avg       0.97      0.97      0.97       200\n",
      "\n",
      "[[18  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0  1]\n",
      " [ 0  0  0  0 23  0  0  0  1  0]\n",
      " [ 0  0  0  0  0 18  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 22  0  0  0]\n",
      " [ 0  0  0  2  0  0  0 18  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 19  0]\n",
      " [ 0  0  0  1  0  0  0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "df_new = df\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for i in range(1000):\n",
    "    df_new['text'][i] = df['text'][i].lower().split(' ')\n",
    "    df_new['text'][i] = [porter.stem(t) for t in df_new['text'][i]]\n",
    "    df_new['text'][i] = ' '.join(df_new['text'][i])\n",
    "\n",
    "#Vector space modelling\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", stop_words='english', decode_error='strict', strip_accents='ascii', \n",
    "                             max_df=0.1, max_features=20000)\n",
    "tfidf_model = vectorizer.fit(df_new[\"text\"])\n",
    "vectors = tfidf_model.transform(df_new[\"text\"])\n",
    "\n",
    "#Modelling\n",
    "x_train = vectors[train_index,:]\n",
    "y_train = df['category'][train_index]  \n",
    "x_test  = vectors[test_index,:] \n",
    "y_test  = df['category'][test_index]   \n",
    "\n",
    "clf = RandomForestClassifier(criterion='gini', n_estimators=400, min_samples_split=10, max_depth=50, max_features='sqrt')\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "    \n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "predictions_10 = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Conducted Steps</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model 1</td>\n",
       "      <td>Count vectorizer</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Model 2</td>\n",
       "      <td>Tf-idf Vectorizer</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Model 3</td>\n",
       "      <td>Count vectorizer + Stopwords removal</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Model 4</td>\n",
       "      <td>Tf-idf Vectorizer + Stopwords removal</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Model 5</td>\n",
       "      <td>Count vectorizer + Stopwords removal + Stemming</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Model 6</td>\n",
       "      <td>Tf-idf Vectorizer + Stopwords removal + Stemming</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Model 7</td>\n",
       "      <td>Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Model 8</td>\n",
       "      <td>Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Model 9</td>\n",
       "      <td>Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Count Vectorizer Parameter Tuning</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Model 10</td>\n",
       "      <td>Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Tf-idf Vectorizer Parameter Tuning</td>\n",
       "      <td>0.970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Models  \\\n",
       "0   Model 1   \n",
       "1   Model 2   \n",
       "2   Model 3   \n",
       "3   Model 4   \n",
       "4   Model 5   \n",
       "5   Model 6   \n",
       "6   Model 7   \n",
       "7   Model 8   \n",
       "8   Model 9   \n",
       "9  Model 10   \n",
       "\n",
       "                                                                                                          Conducted Steps  \\\n",
       "0                                                                                                        Count vectorizer   \n",
       "1                                                                                                       Tf-idf Vectorizer   \n",
       "2                                                                                    Count vectorizer + Stopwords removal   \n",
       "3                                                                                   Tf-idf Vectorizer + Stopwords removal   \n",
       "4                                                                         Count vectorizer + Stopwords removal + Stemming   \n",
       "5                                                                        Tf-idf Vectorizer + Stopwords removal + Stemming   \n",
       "6                                        Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning   \n",
       "7                                       Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning   \n",
       "8    Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Count Vectorizer Parameter Tuning   \n",
       "9  Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Tf-idf Vectorizer Parameter Tuning   \n",
       "\n",
       "   Accuracies  \n",
       "0       0.770  \n",
       "1       0.680  \n",
       "2       0.795  \n",
       "3       0.815  \n",
       "4       0.845  \n",
       "5       0.805  \n",
       "6       0.925  \n",
       "7       0.950  \n",
       "8       0.955  \n",
       "9       0.970  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Models = pd.DataFrame([\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\", \"Model 5\", \n",
    "          \"Model 6\", \"Model 7\", \"Model 8\", \"Model 9\", \"Model 10\"], columns=['Models'])\n",
    "\n",
    "Conducted_Steps = pd.DataFrame([\"Count vectorizer\", \"Tf-idf Vectorizer\", \"Count vectorizer + Stopwords removal\", \n",
    "                   \"Tf-idf Vectorizer + Stopwords removal\", \"Count vectorizer + Stopwords removal + Stemming\", \n",
    "                   \"Tf-idf Vectorizer + Stopwords removal + Stemming\", \n",
    "                   \"Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning\",\n",
    "                   \"Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning\",\n",
    "                   \"Count vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Count Vectorizer Parameter Tuning\",\n",
    "                   \"Tf-idf Vectorizer + Stopwords removal + Stemming + Random Forest Parameter Tuning + Tf-idf Vectorizer Parameter Tuning\"], columns=['Conducted Steps'])\n",
    "\n",
    "Accuracies = pd.DataFrame([predictions_1, predictions_2, predictions_3, predictions_4, predictions_5,\n",
    "             predictions_6, predictions_7, predictions_8, predictions_9, predictions_10], columns=['Accuracies'])\n",
    "\n",
    "comparison  = pd.concat([Models, Conducted_Steps, Accuracies], axis=1)\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
